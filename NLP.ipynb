{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Using Tokenization, Stopword Removal, and Lemmatization\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of separating text into individual words or phrases.\n",
    "\n",
    "- It is the first step in language cleaning.\n",
    "- Tokenization allows the user to work with words as separate objects.\n",
    "\n",
    "> From ChatGPT:\n",
    "> In `spaCy`, tokenization is automatically handled when we create a `Doc` object from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization is the first step in text preprocessing.\"\n",
    "\n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "\n",
    "Stopwords are commonly used words that aren't really needed in sentences when vectorizing, such as \"A\", \"an\", \"the\", \"of\", \"are\" etc.\n",
    "\n",
    "- In spaCy, there is a 'is_stop' attribute that returns a boolean value based off of a library in the spaCy module. \n",
    "- If the boolean is true, it is a stopword, if it is false, it is not a stopword.\n",
    "- This is very useful for removing stopwords in Python using something like a for loop.\n",
    "\n",
    "> From ChatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stopwords: ['Tokenization', 'step', 'text', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords from the tokenized text\n",
    "tokens_without_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Tokens without stopwords:\", tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process of returning a word to its root form. For example, \"running\", \"ran\", and \"runs\" would all be converted into \"run\".\n",
    "\n",
    "- The process of lemmatization allows for normalization of text that helps in analysis and vectorization.\n",
    "- In spaCy, lemmatization is done by using the lemma_ attribute.\n",
    "\n",
    "> From ChatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas: ['tokenization', 'be', 'the', 'first', 'step', 'in', 'text', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing the tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another example by ChatGPT, it is shown using all these three steps together into a single function that preprocesses text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: ['lemmatization', 'stopword', 'removal', 'essential', 'clean', 'text']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Tokenization, Stopword Removal, and Lemmatization\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Example\n",
    "text = \"Lemmatization and stopword removal are essential in cleaning text.\"\n",
    "cleaned_text = clean_text(text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be asking why some of these words like 'removal' aren't changed to something like 'remove'.\n",
    "This is because in lemmatization, part of speech is considered. For example, 'removal' in the context of being a noun will most likely be unchanged, if it is a verb, it will be changed to 'remove'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
