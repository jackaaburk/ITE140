{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arlington2050 Project Summary\n",
    "- Mr. Jones and our class at Arlington Tech participated in the Arlington 2050 project.\n",
    "- People gave their responses on what they think Arlington will be like in 2050.\n",
    "- Specifically, we used the responses from the county fair.\n",
    "- Since we were given many responses, our class used math and coding to process the data into meaningful data and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Process\n",
    "- We specifically used Python to code with the data.\n",
    "- We used a tool called Pandas, which allows Python to interact with Excel files and databases.\n",
    "We can import Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the Excel file and assign it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = pd.read_excel(\"CountyFair.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can rename the columns of the table, making it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = array1.rename(columns={ \"Unnamed: 1\": \"Year2050\", \"Unnamed: 2\": \"Translation1\", \"Unnamed: 3\": \"Getting_Here\",\"Unnamed: 4\": \"Translation2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin processing the data and eventually visualize it.\n",
    "- We can import spacy, a Python library that can process data in the way we want it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some responses were in Spanish, so we used this code to replace them with English translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "string_list = ds['Year2050'].tolist()\n",
    "spanish_list = ds['Translation1'].tolist()\n",
    "IndexCounter = 0\n",
    "for n in spanish_list: # Gets non null values from the spanish translated list, if they're not null then it appends to the corresponding index of the main list.\n",
    "    workingstring = str(n)\n",
    "    if workingstring != 'nan':\n",
    "        string_list[IndexCounter] = workingstring\n",
    "    IndexCounter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to create a word cloud so we can see the most common words among the data.\n",
    "- Firstly, we will put the most common words in a list as their own item.\n",
    "- Whilst putting these words in the list, we filter out stop words (words like An, And, The, etc) and punctuation (!, ?, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ds['Year2050'].str.cat(sep='')\n",
    "doc = nlp(text)\n",
    "words = [token.text for token in doc if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use create a word cloud using a word cloud library we imported earlier with the list we made.\n",
    "> Word cloud code from Alex Elliott."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, mask=None, contour_width=3, contour_color='steelblue').generate(\" \".join(words))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this word cloud, we can see major themes from the responses, such as housing, parks, community, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create histograms to visualize this data.\n",
    "- In our class, we created histograms showing the polarities and subjectivities of the responses (polarity meaning how positive or negative a response was).\n",
    "- First, lets create two lists, one with each of the responses' polarity, and each of the responses' subjectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_list = []\n",
    "sub_list = []\n",
    "t = 0\n",
    "for t in range(2,len(string_list)):\n",
    "    text = string_list[t]\n",
    "    doc = nlp(text)\n",
    "    pol_list.append(doc._.blob.polarity)\n",
    "    sub_list.append(doc._.blob.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can import more libraries that are used to create the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make the subjectivity histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=sub_list)\n",
    "plt.title('Subjectivity of Postcard Responses from County Fair')\n",
    "plt.xlabel('In a range from 0 to 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the polarity histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=pol_list)\n",
    "plt.title('Polarity of Postcard Responses from County Fair')\n",
    "plt.xlabel('In a range from -1 to 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two other methods for data processing and visualization\n",
    "\n",
    "### Dimensionality Reduction (From Alex)\n",
    "- Dimensionality reduction is a method for representing a given dataset using a lower number of features (i.e. dimensions) while still capturing the original data's meaningful properties.\n",
    "- The reason we do this is to remove irrelevant or redundant features, or simply noisy data, to create a model with a lower number of variables.\n",
    "\n",
    "### Vector Embeddings (From Blu)\n",
    "- Vector embedding is a language model technique that maps words to vectors of real numbers\n",
    "- It represents words in vector space in several different dimensions\n",
    "\n",
    "An example seen from Blu, vectorization can be used to find outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # importing spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\") \n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "   print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Though what we learned can be very deep, with some things needed to be going to college for to fully master them, Arlington2050 still taught our class how to take words and sentences, and mathematically represent them. This also teaches us a little about AI, as AI uses vectors to understand words and phrases. In my opinion, this was a great introduction to language processing and data visualization, and actually used real world applications to teach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
